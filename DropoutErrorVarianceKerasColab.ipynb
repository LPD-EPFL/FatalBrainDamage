{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "DropoutErrorVarianceKerasColab.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "[View in Colaboratory](https://colab.research.google.com/github/sergeivolodin/ProbabilisticNeuronFailureCode/blob/master/DropoutErrorVarianceKerasColab.ipynb)"
      ]
    },
    {
      "metadata": {
        "id": "UqbM2LCENx59",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "b9ae9857-cc2f-4af1-89cc-4e21703639f8"
      },
      "cell_type": "code",
      "source": [
        "# keras, np, plt imports\n",
        "import numpy as np\n",
        "import keras\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Dropout\n",
        "from keras.layers.core import Lambda\n",
        "from keras.optimizers import SGD\n",
        "from keras.initializers import Constant\n",
        "from keras import backend as K\n",
        "from keras.utils import plot_model\n",
        "from functools import partial\n",
        "from IPython.display import SVG\n",
        "from keras.utils.vis_utils import model_to_dot\n",
        "from tqdm import tqdm\n",
        "%matplotlib inline\n",
        "from matplotlib import pyplot as plt"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "metadata": {
        "id": "9YlePO9bN2Z8",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def PermanentDropout(p_fail):\n",
        "  \"\"\" Make dropout work when using predict(), not only when using train \"\"\"\n",
        "  return Lambda(lambda x: K.dropout(x, level=p_fail))\n",
        "  \n",
        "def create_model(p_fails, layer_weights, layer_biases, KLips):\n",
        "  \"\"\" Create some simple network with given dropout prob, weights and Lipschitz coefficient for sigmoid \"\"\"\n",
        "  \n",
        "  # checking if length matches\n",
        "  assert(len(p_fails) == len(layer_weights))\n",
        "  assert(len(layer_biases) == len(layer_weights))\n",
        "  \n",
        "  # creating model\n",
        "  model = Sequential()\n",
        "  \n",
        "  # adding layers\n",
        "  for i, (p_fail, w, b) in enumerate(zip(p_fails, layer_weights, layer_biases)):\n",
        "    # is last layer (with output)?\n",
        "    is_last = i + 1 == len(p_fails)\n",
        "    \n",
        "    # adding dense layer with sigmoid for hidden and linear for last layer\n",
        "    model.add(Dense(w.shape[1], input_shape = (w.shape[0], ),\n",
        "                    kernel_initializer = Constant(w),\n",
        "                    #activation = 'linear' if is_last else get_custom_activation(KLips),\n",
        "                    activation = get_custom_activation(KLips),\n",
        "                    bias_initializer = Constant(b)))\n",
        "    \n",
        "    # adding dropout to all layers but last\n",
        "    if not is_last and p_fail > 0:\n",
        "      model.add(PermanentDropout(p_fail))\n",
        "  \n",
        "  # compiling model with some loss and some optimizer (they are unused)\n",
        "  model.compile(loss='binary_crossentropy', optimizer = 'sgd')\n",
        "  #model.summary()\n",
        "  return model\n",
        "\n",
        "# calculate first norm\n",
        "norm1 = partial(np.linalg.norm, ord = 1)\n",
        "\n",
        "# calculate second norm\n",
        "norm2 = partial(np.linalg.norm, ord = 2)\n",
        "\n",
        "def dot_abs(x, y):\n",
        "  \"\"\" Dot product between absolute values of vectors x, y \"\"\"\n",
        "  return np.dot(np.abs(x), np.abs(y))\n",
        "\n",
        "def norm1_minus_dot_abs(x, y):\n",
        "  \"\"\" Product of first norms - dot product between absolute values \"\"\"\n",
        "  return norm1(x) * norm2(y) - dot_abs(x, y)\n",
        "\n",
        "def get_custom_activation(KLips):\n",
        "  \"\"\" Get custom sigmoid activation with given Lipschitz constant \"\"\"\n",
        "  def custom_activation(x):\n",
        "    return K.sigmoid(4 * KLips * x)\n",
        "  return custom_activation\n",
        "\n",
        "class Experiment():\n",
        "  \"\"\" One experiment on neuron crash, contains a fixed weights network \"\"\"\n",
        "  def __init__(self, N, P, KLips):\n",
        "    \"\"\" Initialize using given number of neurons per layer N (array), probability of failure P, and the Lipschitz coefficient \"\"\"\n",
        "    \n",
        "    # checking if the length is correct. Last layer cannot have failures so P is shorter than N\n",
        "    assert(len(N) == len(P) + 1)\n",
        "    \n",
        "    # sqving N, last layer has 1 neuron\n",
        "    self.N = N + [1]\n",
        "    \n",
        "    # saving P, last layer has zero probability of failure\n",
        "    self.P = P + [0]\n",
        "    \n",
        "    # maximal value of output from neuron (1 since using sigmoid)\n",
        "    self.C = 1.\n",
        "    \n",
        "    # saving K\n",
        "    self.K = KLips\n",
        "    \n",
        "    # populating models\n",
        "    self.init_weights()\n",
        "    \n",
        "  def init_weights(self):\n",
        "    \"\"\" Fill in the weights and initialize models \"\"\"\n",
        "    \n",
        "    # array with weight matrices\n",
        "    self.W = []\n",
        "    \n",
        "    # array with biases\n",
        "    self.B = []\n",
        "    \n",
        "    # loop over layers\n",
        "    for i in range(1, len(self.N)):\n",
        "      # creating w and b\n",
        "      w = np.random.randn(self.N[i - 1], self.N[i]) / (self.N[i - 1])\n",
        "      b = np.random.randn(self.N[i]) / self.N[i]\n",
        "      \n",
        "      # adding them to the array\n",
        "      self.W.append(w)\n",
        "      self.B.append(b)\n",
        "      \n",
        "    # creating \"crashing\" model\n",
        "    self.model = create_model(self.P, self.W, self.B, self.K)\n",
        "    \n",
        "    # creating correct model\n",
        "    self.model_no_dropout = create_model([0] * len(self.P), self.W, self.B, self.K)\n",
        "    \n",
        "  def predict_no_dropout(self, data):\n",
        "    \"\"\" Get correct network output for a given input vector \"\"\"\n",
        "    return self.model_no_dropout.predict(np.array([data]))[0][0]\n",
        "  \n",
        "  def predict(self, data, repetitions = 100):\n",
        "    \"\"\" Get crashed network outputs for given input vector and number of repetitions \"\"\"\n",
        "    data = np.repeat(np.array([data]), repetitions, axis = 0)\n",
        "    return self.model.predict(data).T[0]\n",
        "  \n",
        "  def plot_error(experiment, errors):\n",
        "    \"\"\" Plot the histogram of error  \"\"\"\n",
        "    \n",
        "    # plotting\n",
        "    plt.title('Network error histogram plot')\n",
        "    plt.xlabel('Network output error')\n",
        "    plt.ylabel('Frequency')\n",
        "    plt.hist(errors, density = True)\n",
        "    #plt.plot([true, true], [0, 1], label = 'True value')\n",
        "    #plt.legend()\n",
        "    plt.show()\n",
        "  \n",
        "  def get_error(experiment, inp, repetitions = 100):\n",
        "    \"\"\" Return error between crashed and correct networks \"\"\"\n",
        "    return experiment.predict(inp, repetitions = repetitions) - experiment.predict_no_dropout(inp)\n",
        "  \n",
        "  def get_wb(self, layer):\n",
        "    \"\"\" Get weight and bias matrix \"\"\"\n",
        "    return np.vstack((self.W[layer], self.B[layer]))\n",
        "  \n",
        "  def get_max_f(self, layer, func):\n",
        "    \"\"\" Maximize func(weights) over neurons in layer \"\"\"\n",
        "    wb = self.get_wb(layer)\n",
        "    res = [func(w_neuron) for w_neuron in wb.T]\n",
        "    return np.max(res)\n",
        "  \n",
        "  def get_max_f_xy(self, layer, func):\n",
        "    \"\"\" Maximize func(w1, w2) over neurons in layer \"\"\"\n",
        "    wb = self.get_wb(layer)\n",
        "    res = [func(w_neuron1, w_neuron2) for w_neuron1 in wb.T for w_neuron2 in wb.T]\n",
        "    return np.max(res)\n",
        "  \n",
        "  def get_mean_std_error(self):\n",
        "    \"\"\" Get theoretical bound for mean and std of error given weights \"\"\"\n",
        "    \n",
        "    # Expectation of error\n",
        "    EDelta = 0.\n",
        "    \n",
        "    # Expectation of error squared\n",
        "    EDelta2 = 0.\n",
        "    \n",
        "    # Array of expectations\n",
        "    EDeltaArr = [0]\n",
        "    \n",
        "    # Array of expectations of squares\n",
        "    EDelta2Arr = [0]\n",
        "    \n",
        "    # Loop over layers\n",
        "    for layer in range(len(self.W)):\n",
        "      # probability of failure of a single neuron\n",
        "      p_l = self.P[layer]\n",
        "      \n",
        "      # maximal 1-norm of weights\n",
        "      w_1_norm = self.get_max_f(layer, norm1)\n",
        "      \n",
        "      # alpha from article for layer\n",
        "      alpha = self.get_max_f_xy(layer, dot_abs)\n",
        "      \n",
        "      # beta from article for layer\n",
        "      beta = self.get_max_f_xy(layer, norm1_minus_dot_abs)\n",
        "      \n",
        "      # a, b from article for EDelta2 (note that old EDelta is used)\n",
        "      a = self.C ** 2 * p_l * (alpha + p_l * beta) + 2 * self.K * self.C * p_l * (1 - p_l) * beta * EDelta\n",
        "      b = self.K ** 2 * (1 - p_l) * (alpha + (1 - p_l) * beta)\n",
        "      \n",
        "      # Updating EDelta2\n",
        "      EDelta2 = a + b * EDelta2\n",
        "      \n",
        "      # Updating EDelta\n",
        "      EDelta = p_l * w_1_norm * self.C + self.K * w_1_norm * (1 - p_l) * EDelta\n",
        "      \n",
        "      # Adding new values to arrays\n",
        "      EDeltaArr.append(EDelta)\n",
        "      EDelta2Arr.append(EDelta2)\n",
        "      \n",
        "    # Debug output\n",
        "    #print(EDeltaArr)\n",
        "    #print(EDelta2Arr)\n",
        "    \n",
        "    # Returning mean and sqrt(std^2)\n",
        "    return EDelta, EDelta2 ** 0.5"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "a8hFn_eXnYit",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def run_experiment(L = 5, N = 50, p = 0.1, repetitions = 10000, inputs = 50, K = 0.1, do_plot = True, do_print = True, do_tqdm = True):\n",
        "  \"\"\" Run a single experiment with a fixed network \"\"\"\n",
        "  \n",
        "  # Creating an experiment\n",
        "  experiment = Experiment([N] * L, [p] * (L - 1), K)\n",
        "  \n",
        "  # Running the experiment\n",
        "  data = np.random.randn(inputs, N)\n",
        "  tqdm_ = tqdm if do_tqdm else (lambda x : x)\n",
        "  errors = [experiment.get_error(value, repetitions = repetitions) for value in tqdm_(data)]\n",
        "  \n",
        "  # Computing Maximal Absolute Mean/Std Error over \n",
        "  errors_abs = np.abs(errors)\n",
        "  means = np.mean(errors_abs, axis = 1)\n",
        "  stds = np.std(errors_abs, axis = 1)\n",
        "  mean_exp = np.max(means)\n",
        "  std_exp = np.max(stds)\n",
        "  \n",
        "  # Computing bound values\n",
        "  mean_bound, std_bound = experiment.get_mean_std_error()\n",
        "  \n",
        "  # Plotting the error histogram\n",
        "  if do_plot:\n",
        "    experiment.plot_error(np.array(errors).reshape(-1))\n",
        "  \n",
        "  # Printing results summary\n",
        "  if do_print:\n",
        "    print('Error; maximal over inputs, average over dropout:')\n",
        "    print('Experiment %f Std %f' % (mean_exp, std_exp))\n",
        "    print('Equation   %f Std %f' % (mean_bound, std_bound))\n",
        "    print('Tightness  %.1f%% Std %.1f%%' % (100 * mean_exp / mean_bound, 100 * std_exp / std_bound))\n",
        "    \n",
        "  # Returning summary\n",
        "  return mean_exp, std_exp, mean_bound, std_bound"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "eyL8CgrgjHlo",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 404
        },
        "outputId": "d03e01b3-a478-4f3e-ac4b-87ecd60be71c"
      },
      "cell_type": "code",
      "source": [
        "run_experiment(inputs = 5)"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 5/5 [00:02<00:00,  1.85it/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYMAAAEVCAYAAAACW4lMAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nzAwXAzMzw8XAzMxwMTAzM1wMzMwM+P/bgdESRU3AHQAAAABJRU5ErkJggg==\n",
            "text/plain": [
              "<matplotlib.figure.Figure at 0x7fe4c4d3ada0>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Error; maximal over inputs, average over dropout:\n",
            "Experiment 0.001612 Std 0.001263\n",
            "Equation   0.009670 Std 0.003387\n",
            "Tightness  16.7 % Std 37.3 %\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(0.0016123502, 0.0012631454, 0.009670076021484437, 0.003386852041108013)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 22
        }
      ]
    },
    {
      "metadata": {
        "id": "qwQrU0WKj0l8",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}